{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Global imports and settings\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"figure.max_open_warning\"] = -1\n",
    "\n",
    "# Print options\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Slideshow\n",
    "#from traitlets.config.manager import BaseJSONConfigManager\n",
    "#from pathlib import Path\n",
    "\n",
    "#path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
    "#cm = BaseJSONConfigManager(config_dir=str(path))\n",
    "#cm.update(\n",
    "#    \"rise\",\n",
    "#    {\n",
    "#        \"theme\": \"simple\",\n",
    "#        \"transition\": \"zoom\",\n",
    "#        \"start_slideshow_at\": \"selected\",\n",
    "#        \"scroll\": \"true\",   \n",
    "#     }\n",
    "#)\n",
    "\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(\"li\").addClass(\"fragment\");\n",
       "\n",
       "Reveal.addEventListener(\"slidechanged\", function(event){ \n",
       "    $(\"li\").addClass(\"fragment\");\n",
       "    \n",
       "});\n",
       "        \n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$(\"li\").addClass(\"fragment\");\n",
    "\n",
    "Reveal.addEventListener(\"slidechanged\", function(event){ \n",
    "    $(\"li\").addClass(\"fragment\");\n",
    "    \n",
    "});\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img align=\"left\" src=\"https://ri.powerinit.com/Data/UI/RiseLogo_100x120@2x.png\" width=\"10%\"/>\n",
    "\n",
    " <img align=\"right\" src=\"img/scikit-learn-logo.png\" width=\"50%\" />\n",
    "\n",
    "<br /><br /><br /><br /><br /><br /><br/>\n",
    "<h1>An introduction to Machine Learning with Scikit-Learn, Numpy and Pandas*</h1>\n",
    "    <br /><br />\n",
    "    Tomas Olsson\n",
    "    <br/>\n",
    "    January 2019\n",
    "        <br /><br /><br /><br />\n",
    "    <b>Research Institutes of Sweden</b>\n",
    "    <br/>\n",
    "    <b>ICT</b>\n",
    "    <br/>\n",
    "    <b>RISE SICS Västerås (RISE AI)</b>\n",
    "    <br/>\n",
    "    <a href=\"mailto:tomas.olsson@ri.se\">tomas.olsson@ri.se</a>\n",
    "    <br /><br />\n",
    "    <tiny>*Based on a presentation by Gilles Louppe (<a href=\"https://twitter.com/glouppe\">@glouppe</a>), University of Liège</tiny>\n",
    "    <br /><br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prerequisites \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This is an Jupyter notebook, with executable Python code inside\n",
    "42 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Materials available on  <a href=\"https://github.com/RISE-SICS-V\">GitHub</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Require a Python distribution with scientific packages (NumPy, SciPy, Scikit-Learn, Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- See installation instructions in README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tomas Olsson\n",
    "\n",
    "* Senior Machine learning researcher at RISE SICS Västerås\n",
    "* PhD in Computer Science at MdH\n",
    "* Fault diagnostics and anomaly detection\n",
    "* Industrial project partners:\n",
    "    - ABB, Volvo CE, Scania, Atlas Copco, Mälarenergi, BillerudKorsnäs, Tupras, etc.\n",
    "\n",
    "<img align=\"right\" width=\"20%\" src=\"https://avatars3.githubusercontent.com/u/17475878?s=400&u=ff2d8efa1a2996d54d9096d5d32567119e1ac440&v=4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RISE \n",
    "\n",
    "- RISE Research Institutes of Sweden\n",
    "\n",
    "- ~2600 co-workers\n",
    "- 6 divisions: \n",
    "    - Bioeconomy, Bioscience and Materials, ICT, Built Environment, \n",
    "    Safety and Transport, Materials and Production\n",
    "- five business and innovation areas: \n",
    "    - Digitalisation, Energy and Bio-based Economy, Health and Life Science, \n",
    "    Sustainable Cities, and Communities and Mobility.\n",
    "    \n",
    "### RISE SICS VÄSTERÅS\n",
    "- ICT\n",
    "- 45 researchers\n",
    "- Digitalisation\n",
    "- AI/Machine learning\n",
    "- UX design (VR, AR)\n",
    "- Optimisation\n",
    "- Innovation mangement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Outline\n",
    "- Scikit-Learn and the scientific ecosystem in Python\n",
    "- NumPy - the data container\n",
    "- Pandas - the data mangler library\n",
    "- Beyond Scikit-Learn\n",
    "- Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "- Machine learning library written in __Python__\n",
    "- __Simple and efficient__, for both experts and non-experts\n",
    "- Classical, __well-established machine learning algorithms__\n",
    "- Shipped with <a href=\"http://scikit-learn.org/dev/documentation.html\">documentation</a> and <a href=\"http://scikit-learn.org/dev/auto_examples/index.html\">examples</a>\n",
    "- __BSD 3 license__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Community driven development\n",
    "\n",
    "- 20~ core developers (mostly researchers)\n",
    "- 500-1000 occasional contributors\n",
    "- __All working publicly together__ on [GitHub](https://github.com/scikit-learn/scikit-learn)\n",
    "- Emphasis on __keeping the project maintainable__\n",
    "    - Style consistency\n",
    "    - Unit-test coverage\n",
    "    - Documentation and examples\n",
    "    - Code review\n",
    "- Mature and stable\n",
    "- Join us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Python stack for data analysis\n",
    "\n",
    "- The __open source__ Python ecosystem provides __a standalone, versatile and powerful scientific working environment__, including: [NumPy](http://numpy.org), [SciPy](http://scipy.org), [IPython](http://ipython.org), [Matplotlib](http://matplotlib.org), [Pandas](http://pandas.pydata.org/), _and many others..._\n",
    "<center> \n",
    "<img src=\"img/scikit-learn-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/numpy-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/scipy-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/ipython-logo.jpg\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/matplotlib-logo.png\" style=\"max-width: 120px; display: inline\"/>\n",
    "<img src=\"img/pandas-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "</center>\n",
    "- Scikit-Learn builds upon NumPy and SciPy and __complements__ this scientific environment with machine learning algorithms;\n",
    "- By design, Scikit-Learn is __non-intrusive__, easy to use and easy to combine with other libraries;\n",
    "- Core algorithms are implemented in low-level languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data \n",
    "\n",
    "- Input data = Numpy arrays or Scipy sparse matrices ;\n",
    "\n",
    "- Algorithms are expressed using high-level operations defined on matrices or vectors (similar to MATLAB) ;\n",
    "    - Leverage efficient low-leverage implementations ;\n",
    "    - Keep code short and readable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NumPy*\n",
    "\n",
    "- the core library for scientific computing in Python\n",
    "- high-performance multidimensional array objects\n",
    "- tools for working with these arrays\n",
    "- linear algebra: solvers, eigenvalues, etc.\n",
    "- Intro for matlab users: https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html\n",
    "\n",
    "*Based on http://cs231n.github.io/python-numpy-tutorial/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "           \n",
    "# Create the following rank 2 array with shape (3, 4)\n",
    "# [[ 1  2  3  4]\n",
    "#  [ 5  6  7  8]\n",
    "#  [ 9 10 11 12]]\n",
    "a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use slicing to pull out the subarray consisting of the first 2 rows\n",
    "# and columns 1 and 2; b is the following array of shape (2, 2):\n",
    "# [[2 3]\n",
    "#  [6 7]]\n",
    "b = a[:2, 1:3]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A slice of an array is a view into the same data, so modifying it\n",
    "# will modify the original array.\n",
    "print(a[0, 1])   # Prints \"2\"\n",
    "b[0, 0] = 77     # b[0, 0] is the same piece of data as a[0, 1]\n",
    "print(a[0, 1])   # Prints \"77\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=1000, centers=20, random_state=123)\n",
    "labels = [\"b\", \"r\"]\n",
    "y = np.take(labels, (y < 10))\n",
    "print(X) \n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# X is a 2 dimensional array, with 1000 rows and 2 columns\n",
    "print(X.shape)\n",
    " \n",
    "# y is a vector of 1000 elements\n",
    "print(y.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Rows and columns can be accessed with lists, slices or masks\n",
    "print(X[[1, 2, 3]])     # rows 1, 2 and 3\n",
    "print()\n",
    "print(X[:5])            # 5 first rows\n",
    "print()\n",
    "print(X[500:510, 0])    # values from row 500 to row 510 at column 0\n",
    "print()\n",
    "print(X[y == \"b\"][:5])  # 5 first rows for which y is \"b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "   # Plot\n",
    "plt.figure()\n",
    "for label in labels:\n",
    "    mask = (y == label)\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=label)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loading external data\n",
    "\n",
    "- Numpy provides some [simple tools](https://docs.scipy.org/doc/numpy/reference/routines.io.html) for loading data from files (CSV, binary, etc);\n",
    "\n",
    "- For structured data, Pandas provides more [advanced tools](http://pandas.pydata.org/pandas-docs/stable/io.html) (CSV, JSON, Excel, HDF5, SQL, etc); \n",
    "    -  More on Pandas later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algorithms provided by Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Supervised learning:__\n",
    "\n",
    "* Linear models (Ridge, Lasso, Elastic Net, ...)\n",
    "* Support Vector Machines\n",
    "* Tree-based methods (Random Forests, Bagging, GBRT, ...)\n",
    "* Nearest neighbors \n",
    "* Neural networks (basics)\n",
    "* Gaussian Processes\n",
    "* Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Unsupervised learning:__\n",
    "\n",
    "* Clustering (KMeans, Ward, ...)\n",
    "* Matrix decomposition (PCA, ICA, ...)\n",
    "* Density estimation\n",
    "* Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Model selection and evaluation:__\n",
    "\n",
    "* Cross-validation\n",
    "* Grid-search\n",
    "* Lots of metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_... and many more!_ (See our [Reference](http://scikit-learn.org/dev/modules/classes.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Framework\n",
    "\n",
    "Data comes as a finite learning set ${\\cal L} = (X, y)$ where\n",
    "* Input samples are given as an array $X$ of shape `n_samples` $\\times$ `n_features`, taking their values in ${\\cal X}$;\n",
    "* Output values are given as an array $y$, taking _symbolic_ values in ${\\cal Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal of supervised classification is to build an estimator $\\varphi: {\\cal X} \\mapsto {\\cal Y}$ minimizing\n",
    "\n",
    "$$\n",
    "Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}\n",
    "$$\n",
    "\n",
    "where $\\ell$ is a loss function, e.g., the zero-one loss for classification $\\ell_{01}(Y,\\hat{Y}) = 1(Y \\neq \\hat{Y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "- Classifying bad and good signals; \n",
    "- Diagnosing disease from symptoms;\n",
    "- Recognising cats in pictures;\n",
    "- Identifying body parts with Kinect cameras;\n",
    "- ...\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A simple and unified API\n",
    "\n",
    "All learning algorithms in scikit-learn share a uniform and limited API consisting of complementary interfaces:\n",
    "\n",
    "- an `estimator` interface for building and fitting models;\n",
    "- a `predictor` interface for making predictions;\n",
    "- a `transformer` interface for converting data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Goal: enforce a simple and consistent API to __make it trivial to swap or plug algorithms__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits estimator to data.\"\"\"\n",
    "        # set state of ``self``\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the nearest neighbor class\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Change this to try \n",
    "                                                    # something else\n",
    "\n",
    "# Set hyper-parameters, for controlling algorithm\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Learn a model from training data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Estimator state is stored in instance attributes\n",
    "clf._tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions  \n",
    "print(clf.predict(X[:5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute (approximate) class probabilities\n",
    "print(clf.predict_proba(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tutorial import plot_surface    \n",
    "plot_surface(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tutorial import plot_histogram    \n",
    "plot_histogram(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classifier zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision trees\n",
    "\n",
    "Idea: greedily build a partition of the input space using cuts orthogonal to feature axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tutorial import plot_clf\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "Idea: Build several decision trees with controlled randomness and average their decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "clf = RandomForestClassifier(n_estimators=500)\n",
    "# from sklearn.ensemble import ExtraTreesClassifier \n",
    "# clf = ExtraTreesClassifier(n_estimators=500)\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "Idea: model the decision boundary as an hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Support vector machines\n",
    "\n",
    "Idea: Find the hyperplane which has the largest distance to the nearest training points of any class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\")  # try kernel=\"rbf\" instead\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "Idea: a multi-layer perceptron is a circuit of non-linear combinations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation=\"relu\", learning_rate=\"invscaling\")\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gaussian Processes\n",
    "\n",
    "Idea: a gaussian process is a distribution over functions $f$, such that $f(\\mathbf{x})$, for any set $\\mathbf{x}$ of points, is gaussian distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "clf = GaussianProcessClassifier()\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model evaluation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "- Recall that we want to learn an estimator $\\varphi$ minimizing the generalization error $Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}$.\n",
    "\n",
    "- Problem: Since $P_{X,Y}$ is unknown, the generalization error $Err(\\varphi)$ cannot be evaluated.\n",
    "\n",
    "- Solution: Use a proxy to approximate $Err(\\varphi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)   \n",
    "print(\"Training error =\", zero_one_loss(y, clf.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Test error\n",
    "\n",
    "Issue: the training error is a __biased__ estimate of the generalization error.\n",
    "\n",
    "Solution: Divide ${\\cal L}$ into two disjoint parts called training and test sets (usually using 70% for training and 30% for test).\n",
    "- Use the training set for fitting the model;\n",
    "- Use the test set for evaluation only, thereby yielding an unbiased estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training error =\", zero_one_loss(y_train, clf.predict(X_train)))\n",
    "print(\"Test error =\", zero_one_loss(y_test, clf.predict(X_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Summary: Beware of bias when you estimate model performance:\n",
    "- Training score is often an optimistic estimate of the true performance;\n",
    "- __The same data should not be used both for training and evaluation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: \n",
    "- When ${\\cal L}$ is small, training on 70% of the data may lead to a model that is significantly different from a model that would have been learned on the entire set ${\\cal L}$. \n",
    "- Yet, increasing the size of the training set (resp. decreasing the size of the test set), might lead to an inaccurate estimate of the generalization error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solution: K-Fold cross-validation. \n",
    "- Split ${\\cal L}$ into K small disjoint folds. \n",
    "- Train on K-1 folds, evaluate the test error one the held-out fold.\n",
    "- Repeat for all combinations and average the K estimates of the generalization error.\n",
    "<center><img src=\"img/cross-validation.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train, test in KFold(n_splits=5, random_state=42).split(X):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    clf = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
    "    scores.append(zero_one_loss(y_test, clf.predict(X_test)))\n",
    "\n",
    "print(\"CV error = %f +-%f\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shortcut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(KNeighborsClassifier(n_neighbors=5), X, y, \n",
    "                         cv=KFold(n_splits=5, random_state=42), \n",
    "                         scoring=\"accuracy\")\n",
    "print(\"CV error = %f +-%f\" % (1. - np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Default score\n",
    "\n",
    "Estimators come with a built-in default evaluation score\n",
    "* Accuracy for classification \n",
    "* R2 score for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train = (y_train == \"r\")\n",
    "y_test = (y_test == \"r\")\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(X_train, y_train) \n",
    "print(\"Default score =\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "Definition: The accuracy is the proportion of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy =\", accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision, recall and F-measure\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "$$F = \\frac{2 * Precision * Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "print(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\n",
    "print(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\n",
    "print(\"F =\", fbeta_score(y_test, clf.predict(X_test), beta=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ROC AUC\n",
    "\n",
    "Definition: Area under the curve of the false positive rate (FPR) against the true positive rate (TPR) as the decision threshold of the classifier is varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer\n",
    "roc_auc_scorer = get_scorer(\"roc_auc\")\n",
    "print(\"ROC AUC =\", roc_auc_scorer(clf, X_test, y_test))\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Definition: number of samples of class $i$ predicted as class $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model selection\n",
    " \n",
    "- Finding good hyper-parameters is crucial to control under- and over-fitting, hence achieving better performance.\n",
    "- The estimated generalization error can be used to select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under- and over-fitting\n",
    "\n",
    "- Under-fitting: the model is too simple and does not capture the true relation between X and Y.\n",
    "- Over-fitting: the model is too specific to the training set and does not generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Evaluate parameter range in CV\n",
    "param_range = range(2, 200)\n",
    "param_name = \"max_leaf_nodes\"\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    DecisionTreeClassifier(), X, y, \n",
    "    param_name=param_name, \n",
    "    param_range=param_range, cv=5, n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot parameter VS estimated error\n",
    "plt.xlabel(param_name)\n",
    "plt.ylabel(\"score\")\n",
    "plt.xlim(min(param_range), max(param_range))\n",
    "plt.plot(param_range, 1. - train_scores_mean, color=\"red\", label=\"Training error\")\n",
    "plt.fill_between(param_range, \n",
    "                 1. - train_scores_mean + train_scores_std,\n",
    "                 1. - train_scores_mean - train_scores_std,\n",
    "                 alpha=0.2, color=\"red\")\n",
    "plt.plot(param_range, 1. - test_scores_mean, color=\"blue\", label=\"CV error\")\n",
    "plt.fill_between(param_range, \n",
    "                 1. - test_scores_mean + test_scores_std,\n",
    "                 1. - test_scores_mean - test_scores_std, \n",
    "                 alpha=0.2, color=\"blue\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Best trade-off\n",
    "print(\"%s = %d, CV error = %f\" % (param_name,\n",
    "                                  param_range[np.argmax(test_scores_mean)],\n",
    "                                   1. - np.max(test_scores_mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Question: Where is the model under-fitting and over-fitting?\n",
    "\n",
    "Question: What does it mean if the training error is different from the test error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyper-parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(),\n",
    "                    param_grid={\"n_neighbors\": list(range(1, 100))},\n",
    "                    scoring=\"accuracy\",\n",
    "                    cv=5, n_jobs=-1)\n",
    "grid.fit(X, y)  # Note that GridSearchCV is itself an estimator\n",
    "\n",
    "print(\"Best score = %f, Best parameters = %s\" % (1. - grid.best_score_, \n",
    "                                                 grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Question: Should you report the best score as an estimate of the generalization error of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selection and evaluation, _simultaneously_\n",
    "\n",
    "- `grid.best_score_` is not independent from the best model, since its construction was guided by the optimization of this quantity. \n",
    "\n",
    "- As a result, the optimized `grid.best_score_` estimate _may_ in fact be a biased, optimistic, estimate of the true performance of the model. \n",
    "\n",
    "- Solution: Use __nested__ cross-validation for correctly selecting the model __and__ correctly evaluating its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scores = cross_val_score(\n",
    "            GridSearchCV(KNeighborsClassifier(),\n",
    "                         param_grid={\"n_neighbors\": list(range(1, 100))},\n",
    "                         scoring=\"accuracy\",\n",
    "                         cv=5, n_jobs=-1), \n",
    "            X, y, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# Unbiased estimate of the accuracy\n",
    "print(\"%f +-%f\" % (1. - np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformers, pipelines and feature unions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformers\n",
    "\n",
    "- Classification (or regression) is often only one or the last step of a long and complicated process;\n",
    "- In most cases, input data needs to be cleaned, massaged or extended before being fed to a learning algorithm;\n",
    "- For this purpose, Scikit-Learn provides the ``transformer`` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(object):    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits estimator to data.\"\"\"\n",
    "        # set state of ``self``\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X into Xt.\"\"\"\n",
    "        # transform X in some way to produce Xt\n",
    "        return Xt\n",
    "    \n",
    "    # Shortcut\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        Xt = self.transform(X)\n",
    "        return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformer zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits data\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Plot\n",
    "sample_id = 42\n",
    "plt.imshow(X[sample_id].reshape((8, 8)), interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.title(\"y = %d\" % y[sample_id])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scalers and other normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "tf = StandardScaler()\n",
    "tf.fit(X_train, y_train)\n",
    "Xt_train = tf.transform(X_train)  \n",
    "print(\"Mean (before scaling) =\", np.mean(X_train))\n",
    "print(\"Mean (after scaling) =\", np.mean(Xt_train))\n",
    "\n",
    "# Shortcut: Xt = tf.fit_transform(X)\n",
    "# See also Binarizer, MinMaxScaler, Normalizer, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Scaling is critical for some algorithms\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "print(\"Without scaling =\", clf.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"With scaling =\", clf.fit(tf.transform(X_train), y_train).score(tf.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 10 top features, as ranked using ANOVA F-score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "tf = SelectKBest(score_func=f_classif, k=10)\n",
    "Xt = tf.fit_transform(X_train, y_train)\n",
    "print(\"Shape =\", Xt.shape)\n",
    "\n",
    "# Plot support\n",
    "plt.imshow(tf.get_support().reshape((8, 8)), interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature selection (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Feature selection using backward elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "tf = RFE(RandomForestClassifier(), n_features_to_select=10, verbose=1)\n",
    "Xt = tf.fit_transform(X_train, y_train)\n",
    "print(\"Shape =\", Xt.shape)\n",
    "\n",
    "# Plot support\n",
    "plt.imshow(tf.get_support().reshape((8, 8)), interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decomposition, factorization or embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "tf = PCA(n_components=2)\n",
    "Xt_train = tf.fit_transform(X_train)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(Xt_train[:, 0], Xt_train[:, 1], c=y_train)\n",
    "plt.show()\n",
    "\n",
    "# See also: KernelPCA, NMF, FastICA, Kernel approximations, \n",
    "#           manifold learning, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Function transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def increment(X):\n",
    "    return X + 1\n",
    "\n",
    "tf = FunctionTransformer(func=increment)\n",
    "Xt = tf.fit_transform(X)\n",
    "print(X[0])\n",
    "print(Xt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pipelines\n",
    "\n",
    "Transformers can be chained in sequence to form a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Chain transformers to build a new transformer\n",
    "tf = make_pipeline(StandardScaler(), \n",
    "                   SelectKBest(score_func=f_classif, k=10))\n",
    "tf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Xt_train = tf.transform(X_train)\n",
    "print(\"Mean =\", np.mean(Xt_train))\n",
    "print(\"Shape =\", Xt_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Chain transformers + a classifier to build a new classifier\n",
    "clf = make_pipeline(StandardScaler(), \n",
    "                    SelectKBest(score_func=f_classif, k=10), \n",
    "                    RandomForestClassifier())\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.predict_proba(X_test)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters can be accessed using step names\n",
    "print(\"K =\", clf.get_params()[\"selectkbest__k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(clf, \n",
    "                    param_grid={\"selectkbest__k\": [1, 10, 20, 30, 40, 50],\n",
    "                                \"randomforestclassifier__max_features\": [0.1, 0.25, 0.5]})\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params =\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature unions\n",
    "\n",
    "Similarly, transformers can be applied in parallel to transform data in union."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nested composition\n",
    "\n",
    "Since pipelines and unions are themselves estimators, they can be composed into nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "clf = make_pipeline(\n",
    "    # Build features\n",
    "    make_union(\n",
    "        FunctionTransformer(func=lambda X: X), # Identity\n",
    "        PCA(),\n",
    "    ), \n",
    "    # Select the best features\n",
    "    RFE(RandomForestClassifier(), n_features_to_select=10),\n",
    "    # Train\n",
    "    MLPClassifier()\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas\n",
    "\n",
    "<img align=\"right\" src=\"img/pandas-logo.png\" style=\"max-width: 40%; display: inline\" />\n",
    "\n",
    "- Pandas is you friend for managing complex data, especially time series data/signals\n",
    "- Selection by any type not just integers (as numpy)\n",
    "- Hierachical columns/index names\n",
    "- Loads many file formats\n",
    "- Methods for handling missing values\n",
    "- Manges different timezones\n",
    "- Good documentation and examples: <a href=\"https://pandas.pydata.org/\">https://pandas.pydata.org/</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "\n",
    "iris.head(5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris[['petal_length','petal_width']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a hierachical multi index\n",
    "cols = [tuple(_.split('_')) for _ in iris.columns]\n",
    "levels=[[_[0]  for _ in cols ], [_[1]for _ in cols if len(_)>1] + ['']]\n",
    "iris.columns = pd.MultiIndex.from_arrays(levels)\n",
    "iris.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris['petal'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Time series*\n",
    "\n",
    "<small>Download from <a href=\"https://trends.google.com/trends/explore?date=all&q=%2Fm%2F05z1_,%2Fm%2F053_x\">https://trends.google.com/trends/explore?date=all&q=%2Fm%2F05z1_,%2Fm%2F053_x</a></small>\n",
    "\n",
    "<small>*Partially based on <a href=\"https://dzone.com/articles/time-series-data-analysis-tutorial-with-pandas\">https://dzone.com/articles/time-series-data-analysis-tutorial-with-pandas</a></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ts = pd.read_csv('data/multiTimeline.csv', skiprows=1)\n",
    "ts.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ts.Month = pd.to_datetime(ts.Month)\n",
    "ts.set_index('Month', inplace=True)\n",
    "original_cols = ts.columns\n",
    "ts.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can also set timezone and infer the day light saving \n",
    "ts = ts.tz_localize('Europe/Stockholm', ambiguous='infer')\n",
    "ts.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can plot the time series\n",
    "ts.plot(figsize=(10,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can also compute and plot the trend using the moving/rolling mean \n",
    "ts.rolling(12).mean().rename_axis('Year', axis='index').plot(figsize=(10,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can also plot them together\n",
    "ts2 = ts.copy()\n",
    "ts2[[\"Python mean\",\"MATLAB mean\"]] = ts2[original_cols].rolling(12).mean()\n",
    "ts2.plot(figsize=(12,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can also plot them is separate plots\n",
    "a = ts.plot(figsize=(12,6), subplots=True); a[0].set_ylim(bottom=0, top=100); a[1].set_ylim(bottom=0, top=100);\n",
    "ts2['Python mean'].plot(ax=a[0]); ts2['MATLAB mean'].plot(ax=a[1]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Going from Pandas to Scikit-learn via Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X = ts[ts.columns[0]].values.reshape(-1,1)\n",
    "y = ts[ts.columns[1]].values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = iris[['sepal', 'petal']].values\n",
    "y = iris['species']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond Scikit-Learn\n",
    "\n",
    "- Deep learning\n",
    "- Statistical analysis, time series\n",
    "\n",
    "<br/>\n",
    "\n",
    "- Keras (with Tensorflow)\n",
    "- Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Keras\n",
    "\n",
    "- A high level APi that wraps Tensorflow, Theano and CTNK\n",
    "- Designed to be \n",
    "    - Easy to use\n",
    "    - Be modular\n",
    "    - Easy to extend\n",
    "    - Works directly with Python\n",
    "- Take a look for examples and more details: <a href=\"https://keras.io/\">https://keras.io/</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodels\n",
    "\n",
    "- Scikit-learn focus on prediction\n",
    "- Statsmodels focus on classical statistical analysis:\n",
    "    - statistical models\n",
    "    - statistical tests\n",
    "    - statistical data exploration\n",
    "\n",
    "- Examples and more details: <a href=\"https://www.statsmodels.org\">https://www.statsmodels.org</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Scikit-Learn provides essential tools for machine learning. \n",
    "- Numpy is a basic building block\n",
    "- It is more than training classifiers!\n",
    "- It integrates within a larger Python scientific ecosystem\n",
    "- Pandas is your friend!\n",
    "- Keras for deep learning (advanced neural networks)\n",
    "- Statsmodels for classical statistical analysis\n",
    "- Use serach engines to find answers!\n",
    "- Try it for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "rise": {
   "autolaunch": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
